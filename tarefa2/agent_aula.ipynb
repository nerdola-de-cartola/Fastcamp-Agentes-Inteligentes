{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalação das dependências\n",
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Carregamento das variaveis de ambiente (arquivo .env)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in today's natural language processing (NLP) landscape, and their importance can be seen in several aspects:\n",
      "\n",
      "1. **Real-time Applications**: Fast language models enable real-time applications such as chatbots, virtual assistants, and language translation systems to respond quickly and efficiently. This is particularly important in customer-facing applications where delayed responses can lead to frustration and a negative user experience.\n",
      "2. **Low-Latency Inference**: Fast language models can perform inference (i.e., make predictions or generate text) rapidly, which is essential for applications that require immediate responses, such as:\n",
      "\t* Sentiment analysis for social media monitoring\n",
      "\t* Text classification for spam detection\n",
      "\t* Language translation for real-time communication\n",
      "3. **Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of large-scale applications, such as:\n",
      "\t* Processing massive amounts of text data for data analytics\n",
      "\t* Supporting millions of users in a chatbot or virtual assistant\n",
      "\t* Handling high-traffic websites with language-based features\n",
      "4. **Energy Efficiency**: Fast language models can reduce the computational resources required for NLP tasks, leading to energy efficiency and cost savings. This is particularly important for:\n",
      "\t* Edge computing devices with limited power and resources\n",
      "\t* Data centers with large-scale NLP workloads\n",
      "5. **Improved User Experience**: Fast language models can provide a more seamless and responsive user experience, leading to increased user engagement and satisfaction. This is critical for applications such as:\n",
      "\t* Voice assistants like Alexa or Google Assistant\n",
      "\t* Language-based gaming platforms\n",
      "\t* Real-time language translation for travel or business\n",
      "6. **Competitive Advantage**: Fast language models can provide a competitive advantage in industries where speed and responsiveness are critical, such as:\n",
      "\t* Customer service chatbots\n",
      "\t* Real-time sentiment analysis for market research\n",
      "\t* Fast language translation for global businesses\n",
      "7. **Research and Development**: Fast language models can accelerate research and development in NLP, enabling researchers to experiment and iterate more quickly, leading to faster breakthroughs and advancements in the field.\n",
      "8. **Edge AI and IoT**: Fast language models are essential for edge AI and IoT applications, where devices have limited resources and require fast processing to respond to user input or environmental changes.\n",
      "9. **Autonomous Systems**: Fast language models can enable autonomous systems, such as self-driving cars or drones, to process and respond to natural language inputs in real-time, ensuring safe and efficient operation.\n",
      "10. **Future-Proofing**: Fast language models can future-proof NLP applications, allowing them to adapt to increasing data volumes, user demands, and technological advancements, ensuring they remain relevant and effective in the long term.\n",
      "\n",
      "In summary, fast language models are crucial for building responsive, scalable, and efficient NLP applications that can meet the demands of real-time processing, large-scale data, and competitive markets.\n"
     ]
    }
   ],
   "source": [
    "# Cria um cliente da API Groq usando a chave \n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "# Prompt de test para garantir que o modelo está funcionando\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain the importance of fast language models\"}\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Imprime a resposta do modelo\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe que define um agente inteligente\n",
    "class Agent:\n",
    "    def __init__(self, client: Groq, system: str = \"\") -> None:\n",
    "        self.client = client\n",
    "        self.system = system\n",
    "        self.messages: list = []\n",
    "\n",
    "        # Prompt inicial para configuração do modelo\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    # Função que faz o overload do operador (), isso permite que a classe seja chamada como se fosse uma função\n",
    "    # Essa função recebe um prompt, envia esse prompt para o modelo, juntamente com o contexto anterior,\n",
    "    # e recebe a resposta, a resposta também é salva no contexto, o contexto com todas as mensagens está na variável messages\n",
    "    def __call__(self, message=\"\"):\n",
    "        if message:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "    # Envia messages para a api da Groq\n",
    "    def execute(self):\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\", messages=self.messages\n",
    "        )\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt inicial usado para configurar o modelo, esse prompt ensina o LLM sobre como ele deve responder\n",
    "# de acordo com as etapas do loop, ou seja Question, Thought, Action e Pause.\n",
    "# Ele também ensina sobre as actions, ou tools, que estão disponíveis\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 4 * 7 / 3\n",
    "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
    "\n",
    "get_planet_mass:\n",
    "e.g. get_planet_mass: Earth\n",
    "returns weight of the planet in kg\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: What is the mass of Earth times 2?\n",
    "Thought: I need to find the mass of Earth\n",
    "Action: get_planet_mass: Earth\n",
    "PAUSE \n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: 5.972e24\n",
    "\n",
    "Thought: I need to multiply this by 2\n",
    "Action: calculate: 5.972e24 * 2\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this: \n",
    "\n",
    "Observation: 1,1944×10e25\n",
    "\n",
    "If you have the answer, output it as the Answer.\n",
    "\n",
    "Answer: The mass of Earth times 2 is 1,1944×10e25.\n",
    "\n",
    "Now it's your turn:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# Tool para performar operações matemáticas\n",
    "def calculate(operation: str) -> float:\n",
    "    return eval(operation)\n",
    "\n",
    "\n",
    "# Tool para descobrir a massa dos planetas do sistema solar\n",
    "def get_planet_mass(planet) -> float:\n",
    "    match planet.lower():\n",
    "        case \"earth\":\n",
    "            return 5.972e24\n",
    "        case \"jupiter\":\n",
    "            return 1.898e27\n",
    "        case \"mars\":\n",
    "            return 6.39e23\n",
    "        case \"mercury\":\n",
    "            return 3.285e23\n",
    "        case \"neptune\":\n",
    "            return 1.024e26\n",
    "        case \"saturn\":\n",
    "            return 5.683e26\n",
    "        case \"uranus\":\n",
    "            return 8.681e25\n",
    "        case \"venus\":\n",
    "            return 4.867e24\n",
    "        case _:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: I need to find the mass of Earth and Saturn, then add them together and multiply by 2.\n",
      "Thought: I need to find the mass of Earth.\n",
      "Action: get_planet_mass: Earth\n",
      "PAUSE\n",
      "Observation: 5.972e+24\n",
      "Thought: Now I have the mass of Earth, I need to find the mass of Saturn.\n",
      "Action: get_planet_mass: Saturn\n",
      "PAUSE\n",
      "Observation: 5.683e+26\n",
      "Thought: Now I have the mass of Earth and Saturn, I need to add them together.\n",
      "Action: calculate: 5.972e+24 + 5.683e+26\n",
      "PAUSE\n",
      "Observation: 5.74272e+26\n",
      "Thought: Now I have the sum of the masses, I need to multiply it by 2.\n",
      "Action: calculate: 5.74272e+26 * 2\n",
      "PAUSE\n",
      "Observation: 1.148544e+27\n",
      "Answer: The mass of Earth plus the mass of Saturn and all of that times 2 is 1.148544e+27.\n"
     ]
    }
   ],
   "source": [
    "import re # Regex\n",
    "\n",
    "# Loop ReAct\n",
    "def loop(max_iterations=10, query: str = \"\"):\n",
    "\n",
    "    # Criação do agente\n",
    "    agent = Agent(client=client, system=system_prompt)\n",
    "\n",
    "    # Definição das tools, essas strings devem corresponder ao nome de uma função python disponível\n",
    "    tools = [\"calculate\", \"get_planet_mass\"]\n",
    "\n",
    "    next_prompt = query\n",
    "\n",
    "    i = 0\n",
    "  \n",
    "    # Loop com um máximo de iterações para evitar que o agente pense para sempre\n",
    "    while i < max_iterations:\n",
    "        i += 1\n",
    "        # Envia o prompt para o agente, aqui o agente é chamado como uma função, apesar de ser uma classe\n",
    "        result = agent(next_prompt)\n",
    "        print(result)\n",
    "\n",
    "\n",
    "        if \"PAUSE\" in result and \"Action\" in result:\n",
    "            # Se o LLM decidiu tomar uma ação, então execute a ação desejada\n",
    "            action = re.findall(r\"Action: ([a-z_]+): (.+)\", result, re.IGNORECASE)\n",
    "            chosen_tool = action[0][0]\n",
    "            arg = action[0][1]\n",
    "\n",
    "            # O resultado da ação é enviado para o LLM na próxima iteração\n",
    "            if chosen_tool in tools:\n",
    "                # Chama uma das funções definidas em tools\n",
    "                result_tool = eval(f\"{chosen_tool}('{arg}')\")\n",
    "                next_prompt = f\"Observation: {result_tool}\"\n",
    "\n",
    "            else:\n",
    "                next_prompt = \"Observation: Tool not found\"\n",
    "\n",
    "            print(next_prompt)\n",
    "            continue\n",
    "\n",
    "        # Se o LLM julga que uma resposta foi encontrada, então termina o processo\n",
    "        if \"Answer\" in result:\n",
    "            break\n",
    "\n",
    "\n",
    "# Pergunta que inicia o processo\n",
    "loop(query=\"What is the mass of Earth plus the mass of Saturn and all of that times 2?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
